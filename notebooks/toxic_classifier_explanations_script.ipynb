{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1IwvgIyXQ-XEi82HmC9EtqDHN3BAj7ibd",
      "authorship_tag": "ABX9TyO6Is0V7dSnnnWPAoQOmYCF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAfnSU2jPOyn"
      },
      "outputs": [],
      "source": [
        "# install packages\n",
        "!pip install detoxify\n",
        "!pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "from detoxify import Detoxify\n",
        "\n",
        "import lime\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import json\n",
        "\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "PrCF97zzPUCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataframe\n",
        "df = pd.read_csv('drive/MyDrive/University/msci_project/results_toxicity_labels.csv')\n",
        "\n",
        "# df = df[[\"id\",\"comment_text\",\"toxic\"]]\n",
        "df_toxic = df.loc[df['toxicity_label'] == \"toxicity\"]\n",
        "df_nontoxic = df.loc[df['toxicity_label'] == \"non_toxicity\"]"
      ],
      "metadata": {
        "id": "VnHt7ienrWAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(6)"
      ],
      "metadata": {
        "id": "SBoMPQJi6GWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create lime explainer object\n",
        "class_names = ['Non-toxic','Toxic']\n",
        "explainer = LimeTextExplainer(class_names=class_names)"
      ],
      "metadata": {
        "id": "U8ybbWYnjWot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction function\n",
        "def predict(x):\n",
        "  results = Detoxify('original-small').predict(x)[\"toxicity\"]\n",
        "  results_both = np.transpose(np.array([1-np.array(results),results]))\n",
        "  return results_both"
      ],
      "metadata": {
        "id": "UE1XM_mIPfPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_exp_dict(exp, prediction_proba, x, id):\n",
        "  exp = exp.as_list()\n",
        "  output = {}\n",
        "  output[\"id\"] = id\n",
        "  output[\"comment\"] = x\n",
        "  if prediction_proba >= 0.75:\n",
        "    output[\"prediction_proba\"] = round(float(prediction_proba) * 100, 2)\n",
        "    output[\"prediction_label\"] = \"Toxic\"\n",
        "  else:\n",
        "    output[\"prediction_proba\"] = round(float(1-np.array(prediction_proba)) * 100, 2)\n",
        "    output[\"prediction_label\"] = \"Non-toxic\"\n",
        "\n",
        "\n",
        "  output[\"important_words\"] = []\n",
        "  for i in exp:\n",
        "    if i[1] >0:\n",
        "      iw_label = \"Toxic\"\n",
        "    else:\n",
        "      iw_label = \"Non-toxic\"\n",
        "\n",
        "    output[\"important_words\"].append({\"word\":i[0], \"weight\":abs(round(float(i[1]),4)), \"label\":iw_label})\n",
        "\n",
        "  output[\"checked\"] = False\n",
        "  return output"
      ],
      "metadata": {
        "id": "apAlx5QhsBEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_explanations(row, verbose=False):\n",
        "  if verbose:\n",
        "    print(f\"Generating explanations for {row.id}\")\n",
        "  exp = explainer.explain_instance(row.comment_text, predict, num_features=10, num_samples=100)\n",
        "  prediction_proba = row.toxicity\n",
        "  return create_exp_dict(exp, prediction_proba, row.comment_text, row.id)"
      ],
      "metadata": {
        "id": "eotf68fQPnq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df = df.groupby(\"toxicity_label\").sample(n=125, random_state=10).sample(frac = 1)"
      ],
      "metadata": {
        "id": "Iv7lKyIDVytO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate explanation for each row\n",
        "output = []\n",
        "for row in temp_df.itertuples():\n",
        "  num_alpha_chars = len([ele for ele in row.comment_text if ele.isalpha()])\n",
        "\n",
        "  if (1-num_alpha_chars/len(row.comment_text)) < 0.30:\n",
        "    exp = generate_explanations(row, True)\n",
        "    output.append(exp)"
      ],
      "metadata": {
        "id": "FftJ7aAPRhSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# serializing json\n",
        "json_object = json.dumps(output)\n",
        " \n",
        "# writing to sample.json\n",
        "with open(\"explanationData.json\", \"w\") as outfile:\n",
        "    outfile.write(json_object)\n",
        "\n",
        "files.download(\"explanationData.json\") "
      ],
      "metadata": {
        "id": "8dsR4n0AmTKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_i5IfPaE3pUj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}