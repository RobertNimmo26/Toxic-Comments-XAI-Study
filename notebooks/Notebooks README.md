# Notebooks

This folder contains the notebooks used for the study and sampled test dataset from the Jigsaw dataset

## Dataset

`results_toxicity_labels.csv`

This is the result data of toxicity classifier

- The result file consists of around 4K random sampled test data, with around 0.5K samples labeled as 'toxicity' (see the field of toxicity_label)
- The toxicity model is 'original_small' from the detoxify model (https://github.com/unitaryai/detoxify/blob/master/detoxify/detoxify.py), with an AUC of above 98
- The test data is from Toxic Comment Classification Challenge 2018 (https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)
- The toxicity label is determined based on: (1) obscene < 0.75 and identity_attack < 0.75; and (2) toxicity > 0.75 => label=toxicity; otherwise label=non_toxicity

## Explanation generation script

`toxic_classifier_explanations_script.ipynb`

This notebook contains the script used to generate the LIME explanations for our dataset.

The notebook accepts a CSV file as an input, specifically `results_toxicity_labels.csv`, and outputs the explanations as a JSON file, `explanationData.json`.

## Study questionnaire processing script

`process_xai_study_data.ipynb`

This notebook contains the script used to process user study results after completing the Toxic Comment XAI Study.

The notebook accepts a JSON file as an input, specifically `Results.json`, and outputs the processed results as a CSV file, `results_study.csv`.

`Results.json` can be generated by MongoDB by exporting a cluster as a JSON output.

If MongoDB is not used and another database model is, `process_xai_study_data.json` notebook will have to be modified.
