{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3NPRwLfJCp2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZAoBu1-k6G5"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab.data_table import DataTable\n",
    "  DataTable.max_columns = 40\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvfN7ZWJmQMH"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wGVaoeJYtS4"
   },
   "outputs": [],
   "source": [
    "with open('Results.json') as data_file:    \n",
    "    data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wXmdbHS4Yt83",
    "outputId": "159bd694-7d65-4a11-b4cb-ed422feb0ea1"
   },
   "outputs": [],
   "source": [
    "for line in data[:2]:\n",
    "  print(line.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0g-ctnfQfT0i"
   },
   "outputs": [],
   "source": [
    "def personality_questionnaire(questions):\n",
    "  extraversion = (questions[\"tipi1\"] + (8-questions[\"tipi6\"]))/2\n",
    "  agreeableness = ((8-questions[\"tipi2\"]) + questions[\"tipi7\"])/2\n",
    "  conscientiousness = (questions[\"tipi3\"] + (8-questions[\"tipi8\"]))/2\n",
    "  emotional_stability = ((8-questions[\"tipi4\"]) + questions[\"tipi9\"])/2\n",
    "  openness_experiences = (questions[\"tipi5\"] + (8-questions[\"tipi10\"]))/2\n",
    "\n",
    "\n",
    "  return extraversion, agreeableness, conscientiousness, emotional_stability, openness_experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Iw0n3fWoeDj"
   },
   "outputs": [],
   "source": [
    "def trust_questionnaire(questions):\n",
    "  trust1 = questions[\"trust1\"]\n",
    "  trust2 = questions[\"trust2\"]\n",
    "  trust3 = questions[\"trust3\"]\n",
    "  trust4 = questions[\"trust4\"]\n",
    "  trust5 = questions[\"trust5\"]\n",
    "\n",
    "  # Reversed question (I am wary of the [tool])\n",
    "  trust6 = (6-questions[\"trust6\"])\n",
    "\n",
    "  trust7 = questions[\"trust7\"]\n",
    "  trust8 = questions[\"trust8\"]\n",
    "\n",
    "  return trust1+trust2+trust3+trust4+trust5+trust6+trust7+trust8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-72WIWbBtFl"
   },
   "outputs": [],
   "source": [
    "def open_ended_question_mark(question, answer, participants_answer, total_num_marks):\n",
    "  clear_output(wait=True)\n",
    "\n",
    "  display(HTML(f'<h3>{question}</h3>'))\n",
    "\n",
    "  display(HTML(f'{answer}'))\n",
    "\n",
    "  display(HTML(f'<p><b>The answer:</b><p>{\"-----\" * 25}</p><br />'))\n",
    "  display(HTML(f'<p>{participants_answer}</p>'))\n",
    "  display(HTML(f'<br />{\"-----\" * 25}<p><b>End of answer</b><p>'))\n",
    "\n",
    "  mark = -2\n",
    "\n",
    "  while True:\n",
    "    mark_str = input(\"How many marks: \")\n",
    "\n",
    "    try: \n",
    "      mark = float(mark_str)\n",
    "      if mark == -1 or (mark>=0 and mark<=total_num_marks):\n",
    "        break\n",
    "      else:\n",
    "        raise Exception()\n",
    "    except:\n",
    "      print(f\"Mark must be a int/float value in the range 0 - {total_num_marks}. If unsure set to -1.\")\n",
    "\n",
    "  clear_output(wait=True)\n",
    "\n",
    "  return mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6KJJuwnsCMG"
   },
   "outputs": [],
   "source": [
    "def actual_understanding_questionnaire(questions):\n",
    "\n",
    "  actual_understanding_marks = {\n",
    "    \"actualunderstanding1\": 0.0,\n",
    "    \"actualunderstanding2\": 0.0,\n",
    "    \"actualunderstanding3\": 0.0,\n",
    "    \"actualunderstanding4\": 0.0,\n",
    "    \"actualunderstanding5\": 0.0,\n",
    "    \"actualunderstanding6\": 0.0,\n",
    "    \"actualunderstanding7\": 0.0,\n",
    "    \"actualunderstanding8\": 0.0,\n",
    "    \"actualunderstanding9\": 0.0,\n",
    "    \"actualunderstanding10\": 0.0,\n",
    "    \"actualunderstanding11\": 0.0,\n",
    "    \"actualunderstanding12\": 0.0\n",
    "    }\n",
    "\n",
    "\n",
    "  # Question 10\n",
    "  if questions[\"actualunderstanding1\"] == \"c\":\n",
    "    actual_understanding_marks[\"actualunderstanding1\"]=1.0\n",
    "\n",
    "  # Question 11\n",
    "  if questions[\"actualunderstanding2\"] == \"i\":\n",
    "    actual_understanding_marks[\"actualunderstanding2\"]=1.0\n",
    "\n",
    "  # Question 12\n",
    "  if questions[\"actualunderstanding3\"] == \"d\":\n",
    "    actual_understanding_marks[\"actualunderstanding3\"]=1.0\n",
    "\n",
    "  # Question 13\n",
    "  if questions[\"actualunderstanding4\"] == \"c\":\n",
    "    actual_understanding_marks[\"actualunderstanding4\"]=1.0\n",
    "\n",
    "  # Question 14\n",
    "  if questions[\"actualunderstanding5\"] == \"b\":\n",
    "    actual_understanding_marks[\"actualunderstanding5\"]=1.0\n",
    "\n",
    "  # Question 15\n",
    "  question_15 = \"15. You believe the word “Half” is really important to the final prediction as it has Non-toxic connotations, but it hasn’t been included in the top-10 most important words for the comment. What would you do in this situation? (2 mark)\"\n",
    "\n",
    "  answer_15 = '<h3>Open ended answer:</h3><p><ol><li>Answer should include:<ol><li>(1 mark) Add “Half” as a new word</li><li>(1 mark) With the label “Non-toxic” (1/2 mark) and with the word importance score slider set to above 50% (1/2 mark)<ol><li>An exact score is not required in answer but must suggest that you require to set the word importance score slider at least above 50%.</li></ol></li></ol></li></ol></p>'\n",
    "\n",
    "  participants_answer_15 = questions[\"actualunderstanding6\"]\n",
    "\n",
    "  marks_15 = open_ended_question_mark(question_15, answer_15, participants_answer_15, total_num_marks=2)\n",
    "\n",
    "  actual_understanding_marks[\"actualunderstanding6\"] = marks_15\n",
    "\n",
    "  # Question 16\n",
    "  if questions[\"actualunderstanding7\"] == \"e\":\n",
    "    actual_understanding_marks[\"actualunderstanding7\"]=1.0\n",
    "\n",
    "  # Question 17\n",
    "  if questions[\"actualunderstanding8\"] == \"d\":\n",
    "    actual_understanding_marks[\"actualunderstanding8\"]=1.0\n",
    "\n",
    "  # Question 18\n",
    "  if questions[\"actualunderstanding9\"] == \"c\":\n",
    "    actual_understanding_marks[\"actualunderstanding9\"]=1.0\n",
    "\n",
    "  # Question 19\n",
    "  question_19 = \"19. Based only on your experience of completing the previous task and the information which has been provided to you in the instructions, how does the AI system predict that a comment is Toxic or Non-toxic? (3 marks)\"\n",
    "\n",
    "  answer_19 = '<h3>a. Open ended answer. (This should be very high level)</h3><p><ol>  <li>    Answer should include:    <ol>      <li>        (1 mark) Looks at the words in the comment (features)        <ul>          <li>Mentioning using the words will gain the mark</li>        </ul>      </li>      <li>        (1 mark) The AI system works out the significance of each word in the comment (1/2 mark) (word importance score) and if the word is more likely toxic or non-toxic (1/2 mark) (feature label)      </li>      <li>        (1 mark) Two types of responses        <ul>          <li>Comments where there is a higher proportion of significant toxic words to non-toxic words will more likely be classified as Toxic</li><li>Comments where most of the words are not significant will be more likely be classified as Non-toxic</li>        </ul>      </li>    </ol>  </li>  <li>If a participant mentions only training model and the procedure behind that, they will get (0 marks) as we haven’t mentioned this and the technicalities of this during this during the study.    <ul>      <li>I expect participants with more experience in AI will more likely mention this. This will also confirm previous work which has mentioned that expert users are less likely to modify their mental model for a system.</li></ul></li></ol></p>'\n",
    "\n",
    "  participants_answer_19 = questions[\"actualunderstanding10\"]\n",
    "\n",
    "  marks_19 = open_ended_question_mark(question_19, answer_19, participants_answer_19, total_num_marks=3)\n",
    "\n",
    "  actual_understanding_marks[\"actualunderstanding10\"] = marks_19\n",
    "\n",
    "  # Question 20\n",
    "  if questions[\"actualunderstanding11\"] == \"c\":\n",
    "    actual_understanding_marks[\"actualunderstanding11\"]=1.0\n",
    "\n",
    "  # Question 21\n",
    "  question_21 = \"21. Can you explain the justification for your answer to question 20? (1 mark)\"\n",
    "\n",
    "  answer_21 = '<h3>a. Open ended answer</h3><p><ol>  <li>    Answer should include:    <ol>      <li>        (1 mark) Diagram 2’s toxic words have a lower word importance score        <ol>          <li>            Mentioning the opposite case will also gain the mark          </li>        </ol>      </li>    </ol>  </li></ol></p>'\n",
    "\n",
    "  participants_answer_21 = questions[\"actualunderstanding12\"]\n",
    "\n",
    "  marks_21 = open_ended_question_mark(question_21, answer_21, participants_answer_21, total_num_marks=1)\n",
    "\n",
    "  actual_understanding_marks[\"actualunderstanding12\"] = marks_21\n",
    "  \n",
    "\n",
    "  return sum(actual_understanding_marks.values()), actual_understanding_marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CcAM3AY5nrER"
   },
   "outputs": [],
   "source": [
    "def practice_task_check(practice_important_words):\n",
    "  political_object = [word for word in practice_important_words if word[\"word\"] == \"political\"][0]\n",
    "  practice_task_check_result_1, practice_task_check_result_2 = bool(political_object[\"label\"] == \"Toxic\"), bool(political_object[\"weight\"] >= 0.4 and political_object[\"weight\"] <= 0.6)\n",
    "  return practice_task_check_result_1, practice_task_check_result_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5SHO1EzbxDN"
   },
   "outputs": [],
   "source": [
    "def main_task_stats(user_task_data, reset_task_data):\n",
    "  number_comments_checked = 0\n",
    "  number_comment_label_changed = 0\n",
    "  number_iw_label_changed = 0\n",
    "  number_iw_weight_changed = 0\n",
    "  number_new_iw = 0\n",
    "\n",
    "  for comment_i in range(len(user_task_data)):\n",
    "    user_comment = user_task_data[comment_i]\n",
    "    reset_comment = reset_task_data[comment_i]\n",
    "\n",
    "    # If the comment has been checked\n",
    "    if user_comment[\"checked\"]:\n",
    "      number_comments_checked += 1\n",
    "\n",
    "      # If comment label has been changed\n",
    "      if user_comment[\"prediction_label\"] != reset_comment[\"prediction_label\"]:\n",
    "        number_comment_label_changed += 1\n",
    "\n",
    "      # For each IW\n",
    "      for iw_i in range(len(user_comment[\"important_words\"])):\n",
    "        iw_user = user_comment[\"important_words\"][iw_i]\n",
    "        iw_reset = reset_comment[\"important_words\"][iw_i]\n",
    "\n",
    "\n",
    "        if iw_user[\"label\"] != iw_reset[\"label\"]:\n",
    "          number_iw_label_changed += 1\n",
    "\n",
    "        if iw_user[\"weight\"] != iw_reset[\"weight\"]:\n",
    "          number_iw_weight_changed += 1\n",
    "\n",
    "      # Number new IW\n",
    "      number_new_iw += len(user_comment[\"new_important_words\"])\n",
    "\n",
    "  avg_number_changes_per_comment_checked = round((number_comment_label_changed + number_iw_label_changed + number_iw_weight_changed + number_new_iw) / number_comments_checked,2) if number_comments_checked else 0.00\n",
    "\n",
    "  avg_number_comment_label_changes_per_comment_checked = round((number_comment_label_changed) / number_comments_checked,2) if number_comments_checked else 0.00\n",
    "\n",
    "  avg_number_iw_label_changes_per_comment_checked = round((number_iw_label_changed) / number_comments_checked,2) if number_comments_checked else 0.00\n",
    "\n",
    "  avg_number_iw_weight_changes_per_comment_checked = round((number_iw_weight_changed) / number_comments_checked,2) if number_comments_checked else 0.00\n",
    "\n",
    "  avg_number_new_iw_added_per_comment_checked = round((number_new_iw) / number_comments_checked,2) if number_comments_checked else 0.00\n",
    "\n",
    "  return number_comments_checked, number_comment_label_changed, number_iw_label_changed, number_iw_weight_changed, number_new_iw, avg_number_changes_per_comment_checked, avg_number_comment_label_changes_per_comment_checked, avg_number_iw_label_changes_per_comment_checked, avg_number_iw_weight_changes_per_comment_checked, avg_number_new_iw_added_per_comment_checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "CyzIEypTcQjV",
    "outputId": "37802d1f-c25b-44ca-8f4f-e09ec2bbc573"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "start = 0\n",
    "\n",
    "for index, line in enumerate(data[start:]):\n",
    "\n",
    "  print(f\"Comment {index+start+1}\")\n",
    "\n",
    "  input()\n",
    "\n",
    "  # ID details\n",
    "  id = line['_id']['$oid']\n",
    "  prolificID = line['studyInfo']['prolificID']\n",
    "\n",
    "  # Pre-task questionnaire\n",
    "  extraversion, agreeableness, conscientiousness, emotional_stability, openness_experiences = personality_questionnaire(line[\"questionnaire\"][\"preTask\"])\n",
    "  prev_experience = line[\"questionnaire\"][\"preTask\"][\"prevExperience\"]\n",
    "  \n",
    "  # Post-task questionnaire \n",
    "  trust = trust_questionnaire(line[\"questionnaire\"][\"postTask\"])\n",
    "  perceived_understanding = line[\"questionnaire\"][\"postTask\"][\"perceivedunderstanding\"]\n",
    "  total_actual_understanding_marks, detailed_actual_understanding_marks = actual_understanding_questionnaire(line[\"questionnaire\"][\"postTask\"])\n",
    "  actual_understanding_marks_list = [value for value in detailed_actual_understanding_marks.values()]\n",
    "\n",
    "  # Practice task\n",
    "  practice_task_check_result_1, practice_task_check_result_2 = practice_task_check(line[\"taskDataUser\"][\"practice\"][0][\"important_words\"])\n",
    "  \n",
    "  # Main task\n",
    "  number_comments_checked, number_comment_label_changed, number_iw_label_changed, number_iw_weight_changed, number_new_iw, avg_number_changes_per_comment_checked, avg_number_comment_label_changes_per_comment_checked, avg_number_iw_label_changes_per_comment_checked, avg_number_iw_weight_changes_per_comment_checked, avg_number_new_iw_added_per_comment_checked = main_task_stats(line[\"taskDataUser\"][\"main\"], line[\"taskDataReset\"][\"main\"])\n",
    "  \n",
    "  # Add to results\n",
    "  temp = [id, prolificID, practice_task_check_result_1, practice_task_check_result_2, number_comments_checked, number_comment_label_changed, number_iw_label_changed, number_iw_weight_changed, number_new_iw, avg_number_changes_per_comment_checked, avg_number_comment_label_changes_per_comment_checked, avg_number_iw_label_changes_per_comment_checked, avg_number_iw_weight_changes_per_comment_checked, avg_number_new_iw_added_per_comment_checked, extraversion, agreeableness, conscientiousness, emotional_stability, openness_experiences, prev_experience, trust, perceived_understanding, total_actual_understanding_marks]\n",
    "  temp.extend(actual_understanding_marks_list)\n",
    "  results.append(temp)\n",
    "\n",
    "  # Continue or stop\n",
    "  clear_output(wait=True)\n",
    "  ns = input(\"Continue or stop (s)? \")\n",
    "  if ns == \"s\":\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Stopped after completing comment {index+1}\")\n",
    "    break\n",
    "  else:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print(\"Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-km3iEDkjtIB"
   },
   "outputs": [],
   "source": [
    "column_labels = [\"id\", \"prolificID\", \"practice_task_check_result_1\", \"practice_task_check_result_2\", \"number_comments_checked\", \"number_comment_label_changed\", \"number_iw_label_changed\", \"number_iw_weight_changed\", \"number_new_iw\", \"avg_number_changes_per_comment_checked\", \"avg_number_comment_label_changes_per_comment_checked\", \"avg_number_iw_label_changes_per_comment_checked\", \"avg_number_iw_weight_changes_per_comment_checked\", \"avg_number_new_iw_added_per_comment_checked\", \"extraversion\", \"agreeableness\", \"conscientiousness\", \"emotional_stability\", \"openness_experiences\", \"prev_experience\", \"trust\", \"perceived_understanding\", \"total_actual_understanding_marks\", 'actualunderstanding1_mark', 'actualunderstanding2_mark', 'actualunderstanding3_mark', 'actualunderstanding4_mark', 'actualunderstanding5_mark', 'actualunderstanding6_mark', 'actualunderstanding7_mark', 'actualunderstanding8_mark', 'actualunderstanding9_mark', 'actualunderstanding10_mark', 'actualunderstanding11_mark', 'actualunderstanding12_mark']\n",
    "\n",
    "df = pd.DataFrame(results, columns=column_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "DR0xX5IHk2LV",
    "outputId": "75864d2e-4a5a-46fe-b5b1-7653891d7a87"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUUv7-CCejr6"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"results_study.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-aQlLUBKgyX7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
